{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all the \"Performance\" files into one .txt file\n",
    "filenames_time = ['historical_data1_time_Q12015.txt', 'historical_data1_time_Q22015.txt', 'historical_data1_time_Q32015.txt', 'historical_data1_time_Q42015.txt',\n",
    "            'historical_data1_time_Q12016.txt', 'historical_data1_time_Q22016.txt', 'historical_data1_time_Q32016.txt', 'historical_data1_time_Q42016.txt',\n",
    "            'historical_data1_time_Q12017.txt']\n",
    "with open('output_time_file.txt','wb') as wfd:\n",
    "    for f in filenames_time:\n",
    "        with open(f,'rb') as fd:\n",
    "            shutil.copyfileobj(fd, wfd, 1024*1024*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# from pyspark.sql import SQLContext\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "# Load relevant objects\n",
    "# sc = SparkContext('local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we load in the concatenated text file into a PySpark dataframe, which looks like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------+---+---+---+---+---+---+------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|          _1|    _2|       _3| _4| _5| _6| _7| _8| _9|   _10|  _11|_12|_13|_14|_15|_16|_17|_18|_19|_20|_21|_22|_23|_24|\n",
      "+------------+------+---------+---+---+---+---+---+---+------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|F115Q1000001|201504|   320000|  0|  0|180|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201505|   319000|  0|  1|179|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201506|   317000|  0|  2|178|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201507|   316000|  0|  3|177|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201508|   314000|  0|  4|176|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201509|   313000|  0|  5|175|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201510|   311000|  0|  6|174|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201511|309764.76|  0|  7|173|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201512|308289.76|  0|  8|172|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201601|306811.53|  0|  9|171|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201602|305330.07|  0| 10|170|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201603|303845.37|  0| 11|169|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201604|302357.42|  0| 12|168|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201605|300866.22|  0| 13|167|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201606|299371.75|  0| 14|166|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201607|297874.02|  0| 15|165|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201608|296373.01|  0| 16|164|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201609|294868.72|  0| 17|163|   |   |   |      |2.625|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000001|201610|        0|  0| 18|162|  N|   | 01|201610|2.625|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000002|201503|   110000|  0|  0|360|   |   |   |      |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "+------------+------+---------+---+---+---+---+---+---+------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setLogLevel(\"WARN\")\n",
    "log_txt=sc.textFile(\"output_time_file.txt\")\n",
    "temp_var = log_txt.map(lambda k: k.split(\"|\"))\n",
    "header = log_txt.first()\n",
    "log_df=temp_var.toDF()\n",
    "log_df.show()\n",
    "# .split(\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now filtering the rows of dataframe log_df by cases when values in column \"3\" equal 0. New dataframe is called new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---+---+---+---+---+---+---+------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|          _1|    _2| _3| _4| _5| _6| _7| _8| _9|   _10|  _11|_12|_13|_14|_15|_16|_17|_18|_19|_20|_21|_22|_23|_24|\n",
      "+------------+------+---+---+---+---+---+---+---+------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|F115Q1000001|201610|  0|  0| 18|162|  N|   | 01|201610|2.625|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000010|201702|  0|  0| 24|336|  N|   | 01|201702|4.625|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000011|201701|  0|  0| 21|159|  N|   | 01|201701|  3.5|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000012|201608|  0|  0| 17|343|  N|   | 01|201608|3.875|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000017|201705|  0|  0| 26|334|  N|   | 01|201705|3.875|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000020|201609|  0|  0| 17|343|  N|   | 01|201609|3.875|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000021|201701|  0|  0| 23|337|  N|   | 01|201701|  4.5|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000024|201705|  0|  0| 26|333|  N|   | 01|201705| 3.75|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000028|201607|  0|  0| 16|344|  N|   | 01|201607|3.625|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000041|201505|  0|  0|  3|357|  N|   | 01|201505|4.125|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000043|201608|  0|  0| 18|222|  N|   | 01|201608|4.125|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000045|201703|  0|  0| 25|335|  N|   | 01|201703|3.875|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000053|201512|  0|  0| 10|350|  N|   | 01|201512| 4.25|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000059|201505|  0|  0|  3|177|  N|   | 01|201505|    3|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000062|201701|  0|  0| 22|158|  N|   | 01|201701| 2.75|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000069|201609|  0|  0| 19|341|  N|   | 01|201609|4.125|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000072|201609|  0|  0| 19|341|  N|   | 01|201609| 4.75|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000080|201705|  0|  0| 27|153|  N|   | 01|201705| 3.25|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000081|201702|  0|  0| 24|336|  N|   | 01|201702|4.125|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1000084|201608|  0|  0| 18|342|  N|   | 01|201608|4.375|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "+------------+------+---+---+---+---+---+---+---+------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "new_df = log_df.where((col(\"_3\") == \"0\"))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting only the first few columns, because the others are not required for analysis and mostly contain null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---+---+---+---+\n",
      "|          _1|    _2| _3| _4| _5| _6|\n",
      "+------------+------+---+---+---+---+\n",
      "|F115Q1000001|201610|  0|  0| 18|162|\n",
      "|F115Q1000010|201702|  0|  0| 24|336|\n",
      "|F115Q1000011|201701|  0|  0| 21|159|\n",
      "|F115Q1000012|201608|  0|  0| 17|343|\n",
      "|F115Q1000017|201705|  0|  0| 26|334|\n",
      "|F115Q1000020|201609|  0|  0| 17|343|\n",
      "|F115Q1000021|201701|  0|  0| 23|337|\n",
      "|F115Q1000024|201705|  0|  0| 26|333|\n",
      "|F115Q1000028|201607|  0|  0| 16|344|\n",
      "|F115Q1000041|201505|  0|  0|  3|357|\n",
      "|F115Q1000043|201608|  0|  0| 18|222|\n",
      "|F115Q1000045|201703|  0|  0| 25|335|\n",
      "|F115Q1000053|201512|  0|  0| 10|350|\n",
      "|F115Q1000059|201505|  0|  0|  3|177|\n",
      "|F115Q1000062|201701|  0|  0| 22|158|\n",
      "|F115Q1000069|201609|  0|  0| 19|341|\n",
      "|F115Q1000072|201609|  0|  0| 19|341|\n",
      "|F115Q1000080|201705|  0|  0| 27|153|\n",
      "|F115Q1000081|201702|  0|  0| 24|336|\n",
      "|F115Q1000084|201608|  0|  0| 18|342|\n",
      "+------------+------+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = new_df.select([\"_1\",\"_2\",\"_3\",\"_4\",\"_5\",\"_6\"])\n",
    "pdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(DISTINCT _1)|\n",
      "+------------------+\n",
      "|            340128|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "pdf.agg(countDistinct(\"_1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(DISTINCT _1)|\n",
      "+------------------+\n",
      "|           3115090|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "log_df.agg(countDistinct(\"_1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------+---+---+---+---+---+---+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|          _1|    _2|       _3| _4| _5| _6| _7| _8| _9|_10|  _11|_12|_13|_14|_15|_16|_17|_18|_19|_20|_21|_22|_23|_24|\n",
      "+------------+------+---------+---+---+---+---+---+---+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|F115Q1000002|201503|   110000|  0|  0|360|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201504|   110000|  0|  1|359|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201505|   109000|  0|  2|358|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201506|   109000|  0|  3|357|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201507|   109000|  0|  4|356|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201508|   109000|  0|  5|355|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201509|   109000|  0|  6|354|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201510|108656.66|  0|  7|353|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201511|108491.21|  0|  8|352|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201512|108325.23|  0|  9|351|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201601|108158.71|  0| 10|350|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201602|107809.33|  0| 11|349|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201603|107458.82|  0| 12|348|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201604|107107.18|  0| 13|347|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201605|106730.24|  0| 14|346|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201606|106352.08|  0| 15|345|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201607| 105972.7|  0| 16|344|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201608|105592.09|  0| 17|343|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201609|105210.25|  0| 18|342|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201610|104827.18|  0| 19|341|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201611|104442.87|  0| 20|340|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201612|104057.32|  0| 21|339|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201701|103670.53|  0| 22|338|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201702|103282.49|  0| 23|337|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201703| 102893.2|  0| 24|336|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201704|102502.65|  0| 25|335|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201705|102111.65|  0| 26|334|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201706|101719.39|  0| 27|333|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201707|101325.86|  0| 28|332|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201708|100931.06|  0| 29|331|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000002|201709|100534.98|  0| 30|330|   |   |   |   |3.875|  0|   |   |   |   |   |   |   |   |   |   |  0|   |\n",
      "+------------+------+---------+---+---+---+---+---+---+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adf = log_df.where((col(\"_1\") == \"F115Q1000002\"))\n",
    "adf.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------+---+---+---+---+---+---+---+-----+---+------+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|          _1|    _2|       _3| _4| _5| _6| _7| _8| _9|_10|  _11|_12|   _13|_14|_15|_16|_17|_18|_19|_20|_21|_22|_23|_24|\n",
      "+------------+------+---------+---+---+---+---+---+---+---+-----+---+------+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|F115Q1000155|201611|104365.22|  3| 21|339|   |   |   |   |4.125|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000341|201605|380555.33|  3| 15|345|   |   |   |   |4.125|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000373|201609| 49565.82|  3| 19|161|   |   |   |   |  3.5|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000373|201610|  49331.5|  3| 20|160|   |   |   |   |  3.5|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1000954|201612| 83068.58|  3| 22|158|   |   |   |   | 3.75|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1001629|201611| 32925.95|  3| 19|101|   |   |   |   | 3.25|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1001695|201511|293356.96|  3|  9|351|   |   |   |   |    4|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1001755|201604| 43353.04|  3| 14|226|   |   |   |   |    4|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1001930|201605| 75922.94|  3| 14|346|   |   |   |   |    4|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1001930|201608| 75683.94|  3| 17|343|   |   |   |   |    4|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1002402|201611|228862.49|  3| 21|339|   |   |   |   |4.125|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1002402|201612|228510.27|  3| 22|338|   |   |   |   |4.125|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1002402|201701|227089.26|  3| 23|337|   |   |   |   |4.125|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1002494|201702|108581.75|  3| 24|156|   |   |   |   |3.625|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1002494|201709|105879.31|  3| 31|149|   |   |   |   |3.625|  0|201705|   |   |   |   |   |   |   |   |   |  0|   |\n",
      "|F115Q1002619|201701| 45957.24|  3| 23|157|   |   |   |   |    3|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1003432|201708|172131.73|  3| 30|330|   |   |   |   |4.875|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1003905|201601| 73966.99|  3| 11|169|   |   |   |   | 3.75|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1004447|201609|216802.16|  3| 19|341|   |   |   |   |4.125|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "|F115Q1004478|201702|284963.95|  3| 24|335|   |   |   |   | 4.25|  0|      |   |   |   |   |   |   |   |   |   |   |   |\n",
      "+------------+------+---------+---+---+---+---+---+---+---+-----+---+------+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bdf = log_df.where((col(\"_4\") == \"3\"))\n",
    "bdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2996"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`_4`' given input columns: [cnts._1, cnts.cnt];;\\n'Project [_1#0, cnt#769L, row_number() windowspecdefinition('_4, cnt#769L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#774]\\n+- AnalysisBarrier\\n      +- SubqueryAlias cnts\\n         +- Aggregate [_1#0], [_1#0, count(1) AS cnt#769L]\\n            +- LogicalRDD [_1#0, _2#1, _3#2, _4#3, _5#4, _6#5, _7#6, _8#7, _9#8, _10#9, _11#10, _12#11, _13#12, _14#13, _15#14, _16#15, _17#16, _18#17, _19#18, _20#19, _21#20, _22#21, _23#22, _24#23], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o143.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`_4`' given input columns: [cnts._1, cnts.cnt];;\n'Project [_1#0, cnt#769L, row_number() windowspecdefinition('_4, cnt#769L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#774]\n+- AnalysisBarrier\n      +- SubqueryAlias cnts\n         +- Aggregate [_1#0], [_1#0, count(1) AS cnt#769L]\n            +- LogicalRDD [_1#0, _2#1, _3#2, _4#3, _5#4, _6#5, _7#6, _8#7, _9#8, _10#9, _11#10, _12#11, _13#12, _14#13, _15#14, _16#15, _17#16, _18#17, _19#18, _20#19, _21#20, _22#21, _23#22, _24#23], false\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3296)\r\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1307)\r\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2192)\r\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2159)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-49e7139a9a8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m (cnts\n\u001b[1;32m---> 10\u001b[1;33m   \u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rn\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rn\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   .select(\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\"))\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \"\"\"\n\u001b[0;32m   1848\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"col should be Column\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`_4`' given input columns: [cnts._1, cnts.cnt];;\\n'Project [_1#0, cnt#769L, row_number() windowspecdefinition('_4, cnt#769L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#774]\\n+- AnalysisBarrier\\n      +- SubqueryAlias cnts\\n         +- Aggregate [_1#0], [_1#0, count(1) AS cnt#769L]\\n            +- LogicalRDD [_1#0, _2#1, _3#2, _4#3, _5#4, _6#5, _7#6, _8#7, _9#8, _10#9, _11#10, _12#11, _13#12, _14#13, _15#14, _16#15, _17#16, _18#17, _19#18, _20#19, _21#20, _22#21, _23#22, _24#23], false\\n\""
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count, col \n",
    "\n",
    "cnts = log_df.groupBy(\"_1\").agg(count(\"*\").alias(\"cnt\")).alias(\"cnts\")\n",
    "\n",
    "w = Window().partitionBy(\"_4\").orderBy(col(\"cnt\").desc())\n",
    "\n",
    "(cnts\n",
    "  .withColumn(\"rn\", row_number().over(w))\n",
    "  .where(col(\"rn\") == 1)\n",
    "  .select(\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`_4`' given input columns: [cnts._1, cnts.cnt];;\\n'Aggregate [_1#0], [_1#0, max(named_struct(cnt, cnt#801L, NamePlaceholder, '_4)) AS max#808]\\n+- AnalysisBarrier\\n      +- SubqueryAlias cnts\\n         +- Aggregate [_1#0], [_1#0, count(1) AS cnt#801L]\\n            +- LogicalRDD [_1#0, _2#1, _3#2, _4#3, _5#4, _6#5, _7#6, _8#7, _9#8, _10#9, _11#10, _12#11, _13#12, _14#13, _15#14, _16#15, _17#16, _18#17, _19#18, _20#19, _21#20, _22#21, _23#22, _24#23], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o246.agg.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`_4`' given input columns: [cnts._1, cnts.cnt];;\n'Aggregate [_1#0], [_1#0, max(named_struct(cnt, cnt#801L, NamePlaceholder, '_4)) AS max#808]\n+- AnalysisBarrier\n      +- SubqueryAlias cnts\n         +- Aggregate [_1#0], [_1#0, count(1) AS cnt#801L]\n            +- LogicalRDD [_1#0, _2#1, _3#2, _4#3, _5#4, _6#5, _7#6, _8#7, _9#8, _10#9, _11#10, _12#11, _13#12, _14#13, _15#14, _16#15, _17#16, _18#17, _19#18, _20#19, _21#20, _22#21, _23#22, _24#23], false\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.immutable.List.map(List.scala:285)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:66)\r\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:226)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-8ff2bfa5c2f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m (cnts\n\u001b[0;32m      8\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m   \u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cnt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m   .select(col(\"_1\"), col(\"max._4\")))\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"all exprs should be Column\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             jdf = self._jgd.agg(exprs[0]._jc,\n\u001b[1;32m---> 93\u001b[1;33m                                 _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`_4`' given input columns: [cnts._1, cnts.cnt];;\\n'Aggregate [_1#0], [_1#0, max(named_struct(cnt, cnt#801L, NamePlaceholder, '_4)) AS max#808]\\n+- AnalysisBarrier\\n      +- SubqueryAlias cnts\\n         +- Aggregate [_1#0], [_1#0, count(1) AS cnt#801L]\\n            +- LogicalRDD [_1#0, _2#1, _3#2, _4#3, _5#4, _6#5, _7#6, _8#7, _9#8, _10#9, _11#10, _12#11, _13#12, _14#13, _15#14, _16#15, _17#16, _18#17, _19#18, _20#19, _21#20, _22#21, _23#22, _24#23], false\\n\""
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import count, col \n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "cnts = log_df.groupBy(\"_1\").agg(count(\"*\").alias(\"cnt\")).alias(\"cnts\")\n",
    "\n",
    "(cnts\n",
    "  .groupBy(\"_1\")\n",
    "  .agg(F.max(struct(col(\"cnt\"), col(\"_4\"))).alias(\"max\"))\n",
    "  .select(col(\"_1\"), col(\"max._4\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-31345c3e7047>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-31345c3e7047>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    import org.apache.spark.sql.functions.{row_number, max, broadcast}\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{row_number, max, broadcast}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "# val w = Window.partitionBy($\"_1\").orderBy($\"_4\".desc)\n",
    "\n",
    "# val dfTop = df.withColumn(\"rn\", row_number.over(w)).where($\"rn\" === 1).drop(\"rn\")\n",
    "\n",
    "# dfTop.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-b3dda5957ef1>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-b3dda5957ef1>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    dfa = log_df.select($\"_1\", struct($\"_2\", $\"_3\", $\"_4\", $\"_5\").alias(\"vs\"))\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.max\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "from pyspark.sql.functions import struct\n",
    "dfa = log_df.select($\"_1\", struct($\"_2\", $\"_3\", $\"_4\", $\"_5\").alias(\"vs\"))\n",
    "  .groupBy($\"hour\")\n",
    "  .agg(max(\"vs\").alias(\"vs\"))\n",
    "  .select($\"Hour\", $\"vs._2\", $\"vs._3\", $\"vs._4\", $\"vs._5\")\n",
    "\n",
    "dfa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
